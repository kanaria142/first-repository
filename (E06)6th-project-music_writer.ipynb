{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polyphonic-porter",
   "metadata": {},
   "source": [
    "# (E06)6th-project-music_writer\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-maker",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽어오기\n",
    "\n",
    "     Song Lyrics를 https://www.kaggle.com/paultimothymooney/poetry/data 에서 받아 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increased-singles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['', '\\t\\t\\t“There must be some way out of here,” said the joker to the thief', '“There’s too much confusion, I can’t get no relief']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 텐서플로우\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-raise",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제하기\n",
    "      \n",
    "      저장된 문장들을 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adult-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t“There must be some way out of here,” said the joker to the thief\n",
      "“There’s too much confusion, I can’t get no relief\n",
      "Businessmen, they drink my wine, plowmen dig my earth\n",
      "None of them along the line know what any of it is worth”\n",
      "“No reason to get excited,” the thief, he kindly spoke\n",
      "“There are many here among us who feel that life is but a joke\n",
      "But you and I, we’ve been through that, and this is not our fate\n",
      "So let us not talk falsely now, the hour is getting late”\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-sheffield",
   "metadata": {},
   "source": [
    "      정규표현식을 이용해 문장을 다듬어 줍니다. 확인 결과 제대로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closed-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    s = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    s = s.strip()\n",
    "\n",
    "    s = '<start> ' + s + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return s\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-victor",
   "metadata": {},
   "source": [
    "      정제된 문장들을 리스트에 넣어줍니다. 이때 끝이 ':'로 끝나거나 문장이 없는 경우 건너뜁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selected-analysis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some way out of here , said the joker to the thief <end>',\n",
       " '<start> there s too much confusion , i can t get no relief <end>',\n",
       " '<start> businessmen , they drink my wine , plowmen dig my earth <end>',\n",
       " '<start> none of them along the line know what any of it is worth <end>',\n",
       " '<start> no reason to get excited , the thief , he kindly spoke <end>',\n",
       " '<start> there are many here among us who feel that life is but a joke <end>',\n",
       " '<start> but you and i , we ve been through that , and this is not our fate <end>',\n",
       " '<start> so let us not talk falsely now , the hour is getting late <end>',\n",
       " '<start> all along the watchtower , princes kept the view <end>',\n",
       " '<start> while all the women came and went , barefoot servants , too <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interesting-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  65  280   27 ...   10    6 2474]\n",
      " [   2   65   16 ... 5339    3    0]\n",
      " [   2    1    4 ...    3    0    0]\n",
      " ...\n",
      " [   2   75   46 ...    3    0    0]\n",
      " [   2   49    4 ...    0    0    0]\n",
      " [   2   13  636 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fd1a9434850>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "    for length in tensor:\n",
    "        if len(length) > 15:  #문장의 토큰 길이가 15이상일 경우\n",
    "            length.pop()      #pop으로 제거해줍니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen = 15, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "altered-coral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  65  280   27   99   85   56   19   93    4  107    6 6481   10    6\n",
      "  2474]\n",
      " [   2   65   16  103  179 2963    4    5   32   15   43   38 5339    3\n",
      "     0]\n",
      " [   2    1    4   45  470   13  948    4    1  993   13  671    3    0\n",
      "     0]\n",
      " [   2  889   19  110  601    6  431   34   40  397   19   11   26  743\n",
      "     3]\n",
      " [   2   38  568   10   43 2834    4    6 2474    4   55 5041 1763    3\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-richards",
   "metadata": {},
   "source": [
    "      토큰화 된 단어 리스트를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liable-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polish-relaxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  65  280   27   99   85   56   19   93    4  107    6 6481   10    6]\n",
      "[ 280   27   99   85   56   19   93    4  107    6 6481   10    6 2474]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-bandwidth",
   "metadata": {},
   "source": [
    "## 3.  평가 데이터셋 분리\n",
    "\n",
    "      데이터들을 훈련셋과 검증셋으로 나눠줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "behind-strain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consistent-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-position",
   "metadata": {},
   "source": [
    "## 4. 훈련 모델 설계\n",
    "    \n",
    "      모델을 만들고 학습한 후, val_loss와 생성된 문장을 확인합니다.\n",
    "      모델은 RNN을 두 층으로 만들어 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moving-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 # 단어 벡터의 차원수입니다.\n",
    "hidden_size = 1024 # 생각하는 과정 층의 수인데 요약하자면 뱃사공의 수입니다.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-hartford",
   "metadata": {},
   "source": [
    "      모델의 input 모양을 자동으로 결정해주는 코드입니다.\n",
    "      tensor 의 형태가 256, 14, 12001로 각각 batch size와 길이, 단어 수를 뜻합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "typical-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 4.89671984e-05,  1.91120009e-04,  2.52723199e-04, ...,\n",
       "         -5.89945421e-06, -1.57434246e-04, -2.01262053e-04],\n",
       "        [ 5.97140133e-05,  3.17622675e-04,  4.22464305e-04, ...,\n",
       "         -1.71261636e-05, -1.39817537e-04, -4.41581215e-04],\n",
       "        [ 6.01241736e-05,  4.40415752e-04,  5.00677619e-04, ...,\n",
       "          2.89606483e-04, -2.64556991e-04, -4.08167427e-04],\n",
       "        ...,\n",
       "        [ 2.03412492e-04, -6.12976582e-05,  2.41857233e-05, ...,\n",
       "         -1.50883745e-04, -1.74772157e-03, -8.72927485e-04],\n",
       "        [ 2.55839579e-04, -1.75313355e-04, -1.63948251e-04, ...,\n",
       "         -3.50443850e-04, -1.66532316e-03, -8.10133468e-04],\n",
       "        [ 3.61742510e-04, -5.95992242e-05, -1.28375905e-04, ...,\n",
       "         -3.78162717e-04, -1.30274124e-03, -1.05609687e-03]],\n",
       "\n",
       "       [[ 4.89671984e-05,  1.91120009e-04,  2.52723199e-04, ...,\n",
       "         -5.89945421e-06, -1.57434246e-04, -2.01262053e-04],\n",
       "        [ 2.80378561e-04,  2.84652662e-04,  5.27824333e-04, ...,\n",
       "          1.07534579e-04, -3.38626793e-04, -4.87675628e-04],\n",
       "        [ 1.14975235e-04,  9.76176598e-05,  5.31399564e-04, ...,\n",
       "          2.95716454e-04, -3.23896500e-04, -3.98814242e-04],\n",
       "        ...,\n",
       "        [ 3.74312425e-04,  2.99331732e-04, -4.29093518e-04, ...,\n",
       "          1.41625074e-04,  6.88450236e-04,  1.93621472e-05],\n",
       "        [ 7.07758372e-05,  3.69918445e-04, -1.38743519e-04, ...,\n",
       "          6.51178474e-04,  5.84734662e-04,  1.26185434e-04],\n",
       "        [-2.86416936e-04,  4.39020747e-04,  1.75731562e-04, ...,\n",
       "          1.15139165e-03,  4.33739100e-04,  3.15354206e-04]],\n",
       "\n",
       "       [[ 4.89671984e-05,  1.91120009e-04,  2.52723199e-04, ...,\n",
       "         -5.89945421e-06, -1.57434246e-04, -2.01262053e-04],\n",
       "        [ 3.33833523e-05,  3.72233044e-04,  8.05311138e-04, ...,\n",
       "          8.40389475e-05, -2.41217684e-04,  6.11451978e-05],\n",
       "        [ 1.77850874e-04,  1.84397431e-04,  1.02457649e-03, ...,\n",
       "          3.97417261e-06, -1.83772718e-04,  9.16196677e-06],\n",
       "        ...,\n",
       "        [-2.77412095e-04, -6.20475679e-04,  1.42125401e-03, ...,\n",
       "          2.22167186e-03, -2.89475603e-04,  9.13800497e-04],\n",
       "        [-7.44030986e-04, -4.00234188e-04,  1.63255527e-03, ...,\n",
       "          2.47314922e-03, -3.23567772e-04,  1.15587399e-03],\n",
       "        [-1.16110046e-03, -1.71009087e-04,  1.79386919e-03, ...,\n",
       "          2.65142275e-03, -3.76800424e-04,  1.36499712e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.89671984e-05,  1.91120009e-04,  2.52723199e-04, ...,\n",
       "         -5.89945421e-06, -1.57434246e-04, -2.01262053e-04],\n",
       "        [ 1.53760368e-04,  1.36816714e-04,  3.83747130e-04, ...,\n",
       "          1.25509425e-04, -3.44123924e-04,  3.34933793e-05],\n",
       "        [ 1.42843070e-04,  3.39857274e-04,  4.06168489e-04, ...,\n",
       "          2.80395820e-04, -5.24387695e-04, -7.19238960e-05],\n",
       "        ...,\n",
       "        [-1.45821692e-03,  1.01852114e-03,  1.61181903e-03, ...,\n",
       "          2.49177380e-03, -1.26143615e-03,  1.55871047e-03],\n",
       "        [-1.72675552e-03,  1.09363499e-03,  1.70303695e-03, ...,\n",
       "          2.57953955e-03, -1.33246253e-03,  1.69223815e-03],\n",
       "        [-1.97240687e-03,  1.17272593e-03,  1.76849146e-03, ...,\n",
       "          2.63865665e-03, -1.39729807e-03,  1.79682462e-03]],\n",
       "\n",
       "       [[ 4.89671984e-05,  1.91120009e-04,  2.52723199e-04, ...,\n",
       "         -5.89945421e-06, -1.57434246e-04, -2.01262053e-04],\n",
       "        [-1.79583381e-04,  3.26208625e-04,  1.70779022e-04, ...,\n",
       "          1.89242841e-04, -3.21255007e-04, -4.16163588e-04],\n",
       "        [-4.65098390e-04,  1.82257761e-04,  4.02478094e-04, ...,\n",
       "          6.43021078e-04, -4.25560953e-04, -5.50302153e-04],\n",
       "        ...,\n",
       "        [-1.60024536e-03,  5.67039649e-04,  8.01271235e-04, ...,\n",
       "          2.67192279e-03,  2.05619945e-04,  9.52353003e-05],\n",
       "        [-1.80844229e-03,  6.65212865e-04,  9.96036688e-04, ...,\n",
       "          2.91273324e-03,  7.45736324e-05,  4.07885702e-04],\n",
       "        [-2.00108252e-03,  7.72110536e-04,  1.17436622e-03, ...,\n",
       "          3.07850493e-03, -8.44794486e-05,  6.90070039e-04]],\n",
       "\n",
       "       [[ 4.89671984e-05,  1.91120009e-04,  2.52723199e-04, ...,\n",
       "         -5.89945421e-06, -1.57434246e-04, -2.01262053e-04],\n",
       "        [-7.42520788e-05,  2.94294790e-04,  5.63128910e-04, ...,\n",
       "          1.45260856e-04, -3.82519029e-05, -3.16358288e-04],\n",
       "        [-5.40384390e-05,  3.60595935e-04,  5.79926244e-04, ...,\n",
       "          1.97269226e-04,  2.10261962e-04, -2.12220781e-04],\n",
       "        ...,\n",
       "        [-2.39085581e-04,  2.56892294e-03,  7.29276158e-04, ...,\n",
       "          1.98107693e-04, -1.11686706e-03,  1.20176957e-03],\n",
       "        [-4.36796137e-04,  2.36676447e-03,  8.89269810e-04, ...,\n",
       "          6.73603150e-04, -1.04331062e-03,  1.34136470e-03],\n",
       "        [-6.86862448e-04,  2.14766385e-03,  1.07659330e-03, ...,\n",
       "          1.17206865e-03, -9.86477244e-04,  1.50460412e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for enc_train, dec_train in train_dataset.take(1): break\n",
    "model(enc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-picking",
   "metadata": {},
   "source": [
    "      생성된 모델을 확인하고 학습을 시작하고 결과를 관찰해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clear-tuner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "metallic-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 78s 143ms/step - loss: 3.7133 - val_loss: 3.3710\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 79s 145ms/step - loss: 3.2346 - val_loss: 3.1779\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 81s 147ms/step - loss: 3.0532 - val_loss: 3.0497\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 86s 156ms/step - loss: 2.9101 - val_loss: 2.9538\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 85s 155ms/step - loss: 2.7881 - val_loss: 2.8801\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 83s 151ms/step - loss: 2.6777 - val_loss: 2.8171\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 83s 151ms/step - loss: 2.5738 - val_loss: 2.7672\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 80s 146ms/step - loss: 2.4742 - val_loss: 2.7249\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 80s 146ms/step - loss: 2.3803 - val_loss: 2.6872\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 80s 146ms/step - loss: 2.2896 - val_loss: 2.6574\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history_model = model.fit(train_dataset, epochs=10, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-modification",
   "metadata": {},
   "source": [
    "      Epoch가 10회일 때 val loss가 약 2.6으로 목표치인 2.2보다 높습니다.\n",
    "      계속해서 텍스트 생성 함수를 작성하고 생성된 문장을 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "divine-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spread-retailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m gonna be your fantasy <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-depth",
   "metadata": {},
   "source": [
    "      그럴싸한 문장이 나온 것을 확인할 수 있습니다. 계속해서 성능 향상을 시도해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-explorer",
   "metadata": {},
   "source": [
    "## 5. 향상된 학습 모델 설계하기\n",
    "\n",
    "      드롭아웃 추가하기, 배치 정규화, 레이어 수 늘리기 등 여러가지 모델을 설계해보고 학습해 본 결과 아래의 모델의 성능이 가장 뛰어났습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "divided-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus\n",
    "del tensor # 메모리 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accurate-drink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  75513856  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  49168097  \n",
      "=================================================================\n",
      "Total params: 130,826,465\n",
      "Trainable params: 130,826,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "549/549 [==============================] - 349s 635ms/step - loss: 3.7303 - val_loss: 3.2402\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 332s 605ms/step - loss: 3.0773 - val_loss: 2.9548\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 344s 626ms/step - loss: 2.8015 - val_loss: 2.7506\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 332s 604ms/step - loss: 2.5380 - val_loss: 2.5846\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 331s 603ms/step - loss: 2.2492 - val_loss: 2.4490\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 332s 605ms/step - loss: 1.9834 - val_loss: 2.3548\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 332s 605ms/step - loss: 1.7563 - val_loss: 2.3092\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 332s 604ms/step - loss: 1.5786 - val_loss: 2.2906\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 340s 619ms/step - loss: 1.4463 - val_loss: 2.2886\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 332s 605ms/step - loss: 1.3482 - val_loss: 2.2981\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()# 메모리 확보\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.rnn1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True) \n",
    "        self.rnn_d1 = tf.keras.layers.Dropout(0.5)  # 해당 레이어는 드롭아웃입니다\n",
    "        #self.rnn_bn1 = tf.keras.layers.BatchNormalization() #해당 레이어는 배치 정규화입니다\n",
    "        \n",
    "        #self.rnn2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        #self.rnn_d2 = tf.keras.layers.Dropout(0.5)\n",
    "        #self.rnn_bn2 = tf.keras.layers.BatchNormalization() #해당 블록은 추가된 층입니다\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        out = self.rnn1(out)\n",
    "        out = self.rnn_d1(out)\n",
    "        #out = self.rnn_bn1(out)\n",
    "        \n",
    "        #out = self.rnn2(out)\n",
    "        #out = self.rnn_d2(out)\n",
    "        #out = self.rnn_bn2(out)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256*2 # 늘릴수록 소폭의 성능 향상을 확인할 수 있었습니다\n",
    "hidden_size = 1024*4 # 히든 레이어가 많으면 문제가 생길 수 있다고 했지만 확인 결과 늘일수록 학습시간은 오래걸리지만 가장 눈에띄는 성능 향상을 보였습니다\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "for enc_train, dec_train in train_dataset.take(1): break\n",
    "model(enc_train)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history_model = model.fit(train_dataset, epochs=10, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-memphis",
   "metadata": {},
   "source": [
    "      확인 결과 Epoch=10에서 과적응이 보였지만 2.2대에 진입함으로 지금까지 만들었던 모델 중 가장 뛰어난 성능을 보였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "subjective-initial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ancient-superior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m not the only one <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "systematic-advocacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ambient-migration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> one more time fore i go <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> one\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-budget",
   "metadata": {},
   "source": [
    "      문장이 잘 생성되는 것을 확인했습니다...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-titanium",
   "metadata": {},
   "source": [
    "***\n",
    "# 결론 및 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-greek",
   "metadata": {},
   "source": [
    "      해당 모델을 설계하고 성능을 확인하는 동안 학습시간이 너무 오래 걸려서 시간의 낭비가 심했다. 게다가 층을 늘리든, 어떠한 정규화와 드롭아웃을 추가해줘도 과적응이 일어나거나 성능의 향상이 미미해 목표치에 미달하는 경우가 많았다. 결국 해당 예제에서 추천하지 않은 히든 레이어 수를 늘리는 것이 가장 효과적이였다.\n",
    "      \n",
    "      느끼기에는 학습모델의 val loss값이 어떻든 생성되는 문장의 수준은 비슷비슷한 거 같아서 아쉬웠다. 심지어 문장이 통채로 어떤 노래의 가사였던 경우도 있었다. 전체적으로 들인 시간과 노력에 비해 향상은 보이지 않고 지루해서 피곤한 과정이였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
